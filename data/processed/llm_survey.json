{
  "ABSTRACT": "ive summarization,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 4034\u20134048. S. Moon, P. Shah, A. Kumar, and R. Subba, \u201cOpen- dialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs,\u201d in ACL (1). Association for Computational Linguistics, 2019, pp. 845\u2013854. Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettle- moyer, S. W. Yih, D. Fried, S. I. Wang, and T. Yu, \u201cDS-1000: A natural and reliable benchmark for data science code generation,\u201d CoRR, vol. abs/2211.11501, 2022. Z. Wang, S. Zhou, D. Fried, and G. Neubig, \u201cExecution-based evaluation for open-domain code 124 generation,\u201d CoRR, vol. abs/2212.10481, 2022. T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kel- cey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov, \u201cNatural questions: a benchmark for ques- tion answering research,\u201d Trans. Assoc. Comput. Lin- guistics, pp. 452\u2013466, 2019. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, \u201cThink you have solved question answering? try arc, the AI2 reasoning chal- lenge,\u201d CoRR, vol. abs/1803.05457, 2018. S. Lin, J. Hilton, and O. Evans, \u201cTruthfulqa: Measur- ing how models mimic human falsehoods,\u201d in Pro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022, pp. 3214\u20133252. J. Berant, A. Chou, R. Frostig, and P. Liang, \u201cSemantic parsing on freebase from question-answer pairs,\u201d in Proceedings of the 2013 Conference on Empirical",
  "INTRODUCTION": "and advanced",
  "METHODS": "in Natural Language Processing, 2023, pp. 13 358\u201313 376. T. Xu, S. Wu, S. Diao, X. Liu, X. Wang, Y. Chen, and J. Gao, \u201cSayself: Teaching llms to express con- fidence with self-reflective rationales,\u201d arXiv preprint arXiv:2405.20974, 2024. A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Ha- jishirzi, \u201cSelf-rag: Learning to retrieve, generate, and critique through self-reflection,\u201d arXiv preprint 143 arXiv:2310.11511, 2023. H. Luo, Y.-S. Chuang, Y. Gong, T. Zhang, Y. Kim, X. Wu, D. Fox, H. Meng, and J. Glass, \u201cSail: Search- augmented instruction learning,\u201d arXiv preprint arXiv:2305.15225, 2023. X. V. Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Rodriguez, J. Kahn, G. Szilvasy, M. Lewis et al., \u201cRa-dit: Retrieval-augmented dual instruction tuning,\u201d arXiv preprint arXiv:2310.01352, 2023. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, \u201cRetrieval augmented language model pre-training,\u201d in International conference on machine learning. PMLR, 2020, pp. 3929\u20133938. K. Lee, M.-W. Chang, and K. Toutanova, \u201cLatent re- trieval for weakly supervised open domain question answering,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 6086\u20136096. J. Li, J. Chen, R. Ren, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, \u201cThe dawn after the dark: An empirical study on factuality hallucination in large language models,\u201d arXiv preprint arXiv:2401.03205, 2024. Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, \u201cSurvey of hallucination in natural language generation,\u201d ACM Comput. Surv., 2023. Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi, F. Shi, and S. Shi, \u201cSiren\u2019s song in the AI ocean: A survey on hallucination in large language models,\u201d arXiv preprint arXiv:2309.01219, 2023. S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, \u201cScheduled sampling for sequence prediction with recurrent neural networks,\u201d in NIPS, 2015, pp. 1171\u2013 1179. M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, N. Cheng, E. Dur- mus, Z. Hatfield-Dodds, S. R. Johnston, S. Kravec, T. Maxwell, S. McCandlish, K. Ndousse, O. Rausch, N. Schiefer, D. Yan, M. Zhang, and E. Perez, \u201cTo- wards understanding sycophancy in language mod- els,\u201d CoRR, vol. abs/2310.13548, 2023. V. Rawte, P. Priya, S. M. T. I. Tonmoy, S. M. M. Zaman, A. P. Sheth, and A. Das, \u201cExploring the re- lationship between LLM hallucinations and prompt linguistic nuances: Readability, formality, and con- creteness,\u201d CoRR, vol. abs/2309.11064, 2023. S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, \u201cChain-of-verification reduces hallucination in large language models,\u201d CoRR, vol. abs/2309.11495, 2023. P. Manakul, A. Liusie, and M. J. F. Gales, \u201cSelfcheck- gpt: Zero-resource black-box hallucination detection for generative large language models,\u201d in EMNLP. Association for Computational Linguistics, 2023, pp. 9004\u20139017. N. Varshney, W. Yao, H. Zhang, J. Chen, and D. Yu, \u201cA stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation,\u201d CoRR, vol. abs/2307.03987, 2023. Y. Yehuda, I. Malkiel, O. Barkan, J. Weill, R. Ronen, and N. Koenigstein, \u201cIn search of truth: An interro- gation approach to hallucination detection,\u201d CoRR, vol. abs/2403.02889, 2024. S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, \u201cFactscore: Fine-grained atomic evaluation of factual precision in long form text generation,\u201d 2023. I. Chern, S. Chern, S. Chen, W. Yuan, K. Feng, C. Zhou, J. He, G. Neubig, and P. Liu, \u201cFactool: Factuality detection in generative AI - A tool aug- mented framework for multi-task and multi-domain scenarios,\u201d CoRR, vol. abs/2307.13528, 2023. X. Cheng, J. Li, W. X. Zhao, H. Zhang, F. Zhang, D. Zhang, K. Gai, and J.-R. Wen, \u201cSmall agent can also rock! empowering small language models as hallucination detector,\u201d CoRR, vol. abs/2406.11277, 2024. M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, E. Durmus, Z. Hatfield- Dodds, S. R. Johnston, S. Kravec, T. Maxwell, S. Mc- Candlish, K. Ndousse, O. Rausch, N. Schiefer, D. Yan, M. Zhang, and E. Perez, \u201cTowards understanding sycophancy in language models,\u201d in ICLR. Open- Review.net, 2024. J. W. Wei, D. Huang, Y. Lu, D. Zhou, and Q. V. Le, \u201cSimple synthetic data reduces sycophancy in large language models,\u201d CoRR, vol. abs/2308.03958, 2023. L. Gao, Z. Dai, P. Pasupat, A. Chen, A. T. Chaganty, Y. Fan, V. Y. Zhao, N. Lao, H. Lee, D. Juan, and K. Guu, \u201cRARR: researching and revising what lan- guage models say, using language models,\u201d in ACL (1). Association for Computational Linguistics, 2023, pp. 16 477\u201316 508. R. Zhao, X. Li, S. Joty, C. Qin, and L. Bing, \u201cVerify- and-edit: A knowledge-enhanced chain-of-thought framework,\u201d in ACL (1). Association for Compu- tational Linguistics, 2023, pp. 5823\u20135840. H. Trivedi, N. Balasubramanian, T. Khot, and A. Sab- harwal, \u201cInterleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step ques- tions,\u201d CoRR, vol. abs/2212.10509, 2022. K. Li, O. Patel, F. B. Vi\u00b4egas, H. Pfister, and M. Watten- berg, \u201cInference-time intervention: Eliciting truthful answers from a language model,\u201d in NeurIPS, 2023. W. Shi, X. Han, M. Lewis, Y. Tsvetkov, L. Zettlemoyer, and S. W. Yih, \u201cTrusting your evidence: Halluci- nate less with context-aware decoding,\u201d CoRR, vol. abs/2305.14739, 2023. D. Kahneman, \u201cThinking, fast and slow,\u201d Farrar, Straus and Giroux, 2011. S. Wu, Z. Peng, X. Du, T. Zheng, M. Liu, J. Wu, J. Ma, Y. Li, J. Yang, W. Zhou et al., \u201cA comparative study on reasoning patterns of openai\u2019s o1 model,\u201d arXiv preprint arXiv:2410.13639, 2024. T. Zhong, Z. Liu, Y. Pan, Y. Zhang, Y. Zhou, S. Liang, Z. Wu, Y. Lyu, P. Shu, X. Yu et al., \u201cEvaluation of openai o1: Opportunities and challenges of agi,\u201d arXiv preprint arXiv:2409.18486, 2024. Y. Min, Z. Chen, J. Jiang, J. Chen, J. Deng, Y. Hu, Y. Tang, J. Wang, X. Cheng, H. Song et al., \u201cImitate, 144 explore, and self-improve: A reproduction report on slow-thinking reasoning systems,\u201d arXiv preprint arXiv:2412.09413, 2024. D. Team, \u201cDeepseek-r1-lite-preview is now live: un- leashing supercharged reasoning power,\u201d 2024. Q. Team, \u201cQwq: Reflect deeply on the boundaries of the unknown, november 2024,\u201d URL https://qwenlm. github. io/blog/qwq-32b-preview. DeepSeek-AI, \u201cDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\u201d 2025. J. Jiang, Z. Chen, Y. Min, J. Chen, X. Cheng, J. Wang, Y. Tang, H. Sun, J. Deng, W. X. Zhao, Z. Liu, D. Yan, J. Xie, Z. Wang, and J.-R. Wen, \u201cEnhancing llm rea- soning with reward-guided tree search,\u201d 2024. T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, S. Shi, and Z. Tu, \u201cEncouraging divergent thinking in large language models through multi- agent debate,\u201d arXiv preprint arXiv:2305.19118, 2023. Y. Du, Z. Liu, Y. Li, W. X. Zhao, Y. Huo, B. Wang, W. Chen, Z. Liu, Z. Wang, and J.-R. Wen, \u201cVirgo: A preliminary exploration on reproducing o1-like mllm,\u201d arXiv preprint arXiv:2501.01904, 2025. K. Team, \u201cKimi k1.5: Scaling reinforcement learning with llms,\u201d 2025. [Online]. Available: https://arxiv. org/abs/2501.12599 OpenAI, \u201cOpenai\u2019s reinforcement fine-tuning re- search program,\u201d OpenAI Blog, 2024. Z. Zeng, Q. Cheng, Z. Yin, B. Wang, S. Li, Y. Zhou, Q. Guo, X. Huang, and X. Qiu, \u201cScaling of search and learning: A roadmap to reproduce o1 from reinforcement learning perspective,\u201d arXiv preprint arXiv:2412.14135, 2024. Z. Chen, Y. Min, B. Zhang, J. Chen, J. Jiang, D. Cheng, W. X. Zhao, Z. Liu, X. Miao, Y. Lu, L. Fang, Z. Wang, and J.-R. Wen, \u201cAn empirical study on eliciting and improving r1-like reasoning models,\u201d 2025. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., \u201cDeepseek- math: Pushing the limits of mathematical rea- soning in open language models,\u201d arXiv preprint arXiv:2402.03300, 2024. W. Kool, H. van Hoof, and M. Welling, \u201cBuy 4 REIN- FORCE samples, get a baseline for free!\u201d in Deep Re- inforcement Learning Meets Structured Prediction, ICLR 2019 Workshop, New Orleans, Louisiana, United States, May 6, 2019. OpenReview.net, 2019. C. Snell, J. Lee, K. Xu, and A. Kumar, \u201cScaling llm test-time compute optimally can be more effective than scaling model parameters,\u201d 2024. [Online]. Available: https://arxiv.org/abs/2408.03314 W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie, Y. Li, B. Ding, and J. Zhou, \u201cFederatedscope- llm: A comprehensive package for fine-tuning large language models in federated learning,\u201d 2023.",
  "RESULTS": "of the Five-Year Research Effort at Carnegie-Mellon University, 1977. P. Koehn and R. Knowles, \u201cSix challenges for neural machine translation,\u201d in NMT@ACL. Association for Computational Linguistics, 2017, pp. 28\u201339. Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, 114 K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. Cor- rado, M. Hughes, and J. Dean, \u201cGoogle\u2019s neural machine translation system: Bridging the gap be- tween human and machine translation,\u201d CoRR, vol. abs/1609.08144, 2016. R. Paulus, C. Xiong, and R. Socher, \u201cA deep re- inforced model for",
  "DISCUSSION": "about advanced topics, including long context modeling, LLM- based agent, analysis and optimization for training and inference, model inference, model compression, retrieval-augmented generation, and hallucination. \u2022 Update on October 12, 2024: \u2013 Section 8.1.5: correct the errors. \u2022 Update on March 11, 2025: \u2013 Section 9.8: add latest papers about long CoT rea- soning, including the analysis of reasoning patterns and advantages, construction of long CoT data (i.e., distillation, search-based, and multi-agent collabora- tion), and training",
  "REFERENCES": ",\u201d CoRR, vol. abs/2303.07610, 2023. Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu, K. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou, \u201cBenchmarking foundation models with language- model-as-an-examiner,\u201d CoRR, vol. abs/2306.04181, 2023. Y. Liu, S. Feng, D. Wang, Y. Zhang, and H. Sch\u00a8utze, \u201cEvaluate what you can\u2019t evaluate: Unassess- able generated responses quality,\u201d CoRR, vol. abs/2305.14658, 2023. P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui, \u201cLarge language models are not fair evaluators,\u201d CoRR, vol. abs/2305.17926, 2023. J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen, J. Zhou, S. Chen, T. Gui, Q. Zhang, and X. Huang, \u201cA comprehensive capabil- ity analysis of gpt-3 and gpt-3.5 series models,\u201d arXiv preprint arXiv:2303.10420, 2023. M. McCloskey and N. J. Cohen, \u201cCatastrophic in- terference in connectionist networks: The sequential learning problem,\u201d in Psychology of learning and moti- vation, 1989, pp. 109\u2013165. R. Kemker, M. McClure, A. Abitino, T. L. Hayes, and C. Kanan, \u201cMeasuring catastrophic forgetting in neural networks,\u201d in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Or- leans, Louisiana, USA, February 2-7, 2018, 2018, pp. 3390\u20133398. T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C. Wu, M. Zhong, P. Yin, S. I. Wang, V. Zhong, B. Wang, C. Li, C. Boyle, A. Ni, Z. Yao, D. Radev, C. Xiong, L. Kong, R. Zhang, N. A. Smith, L. Zettlemoyer, and T. Yu, \u201cUnifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models,\u201d in EMNLP. Associ- ation for Computational Linguistics, 2022, pp. 602\u2013 631. A. Roberts, C. Raffel, and N. Shazeer, \u201cHow much knowledge can you pack into the parameters of a lan- guage model?\u201d in Proceedings of the 2020 Conference on Empirical",
  "CONCLUSION": "AND FUTURE DIRECTIONS In this survey, we have reviewed the recent progress of large language models (LLMs), and introduced the key concepts, findings, and techniques for understanding and utilizing LLMs. We focus on the large-sized models (i.e., having a size larger than 10B) while excluding the contents of early pre- trained language models (e.g., BERT and GPT-2) that have been well covered in the existing literature. In particular, our survey has discussed four important aspects of LLMs, i.e., pre-training, adaptation, utilization, and evaluation. For each aspect, we highlight the techniques or findings that are key to the success of LLMs. Furthermore, we also summa- rize the available resources for developing LLMs and dis- cuss important implementation guidelines for reproducing LLMs. This survey tries to cover the most recent literature about LLMs and provides a good reference resource on this topic for both researchers and engineers. Next, we summarize the",
  "METHODOLOGY": "and model type, which are further extended with the test abilities. There are some recent work [735, 736] that also has discussed the categorization or taxonomies of existing work for LLM evaluation. 7.4 Empirical Evaluation The above evaluation benchmarks and approaches are mainly employed to evaluate the overall abilities of LLMs. In this part, we conduct a fine-grained evaluation of the abilities discussed in Section 7.1 and Section 7.2. For each kind of ability, we select representative tasks and datasets for conducting evaluation experiments to examine the cor- responding performance of LLMs. 7.4.1 Experimental Settings In this part, we introduce the experimental settings for our evaluation. Evaluation Models. To conduct the evaluation, we consider representative LLMs from open-source models to closed- source API-accessing models as follows: \u2022 Open-source models. Existing open-source models can be categorized into base models and instruction-tuned models. Base models are only pre-trained on a large general-purpose corpus with the language modeling objective, but without further supervised fine-tuning. In our evaluation, we select four representative base models including LLaMA (7B) , LLaMA 2 (7B) , Pythia (7B and 12B) , and Falcon (7B) 41. Instruction-tuned models are those fine-tuned using instructions (i.e., task datasets, daily chat, or syn- thetic instructions). In our experiments, we select four rep- resentative instruction-tuned models including Vicuna (7B and 13B) , Alpaca (7B) , and ChatGLM (6B) . In addition, we also include LLaMA 2-Chat (7B) for comparison, and it is a representative model that has been aligned with human via instruction tuning and RLHF, based on LLaMA 2 (7B). \u2022 Closed-source models. In addition to the open-source models, there are also closed-source models that can only be accessed via APIs, which have gained much attention from both developers and researchers. Here, we select four representative closed-source models including text-davinci- 002/003 (short as Davinci002/003), ChatGPT, Claude, and 41. Experiments with larger models are still in schedule due to the limit of computational resources. Claude 2, where the first three models are developed by OpenAI and the other two are developed by Anthropic. Tasks and Datasets. Next, we set up the evaluation tasks and datasets for the abilities discussed in Section 7.1 and Section 7.2. We mainly evaluate the zero-shot performance of LLMs on these datasets. For more complex tasks that are hard to be solved in the zero-shot manner (e.g., mathemati- cal reasoning and tool manipulation), we mainly report the 3-shot performance, considering the context length limit of open-source models. \u2022 Language generation. As discussed before, for language generation, we consider evaluating three kinds of tasks, i.e., language modeling, conditional text generation, and code synthesis. Specially, we select four commonly-used datasets, namely LAMBADA (language modeling), WMT\u201922 (machine translation), XSum (text sum- marization), and HumanEval (code synthesis) for eval- uation. In WMT\u201922, we construct a new evaluation set by selecting 1000 examples for each language pair from the original large-scale test set to examine the average performance of LLMs in machine translation. We evaluate the zero-shot performance of LLMs on these datasets, and compute the accuracy of predicting words for LAMBADA, BLEU-4 for WMT\u201922, ROUGE-L for XSum, and pass@10 for HumanEval. \u2022 Knowledge utilization. To evaluate the ability of knowl- edge utilization, we select four question answering datasets (i.e., TriviaQA , Natural Questions , Web Ques- tions , and ARC ), and a fact extraction dataset, WikiFact . We also report the zero-shot performance of LLMs on these datasets, and compute accuracy for ARC and exact match for other datasets. \u2022 Complex reasoning. For complex reasoning, we eval- uate the comparison models on OpenbookQA , Hel- laSwag , and SocialIQA for knowledge reason- ing; Colored Objects and Penguins in the Table for symbolic reasoning; GSM8k and MATH for mathematical reasoning. We compute the accuracy for Open- bookQA, HellaSwag, and SocialIQA; solve rate for Colored Objects and Penguins in the Table; and accuracy for GSM8k and MATH. For knowledge reasoning tasks, we evaluate the zero-shot performance, since they are all QA tasks that can be solved in a zero-shot setting. For complex symbolic reasoning and mathematical reasoning tasks, we leverage 3-shot in-context exemplars to better elicit LLMs to accom- plish them. Following existing work [33, 436], we also utilize the chain-of-thought prompting strategy for better solving the mathematical reasoning tasks. \u2022 Human alignment. For human alignment, we select TruthfulQA to measure whether a LLM is truth- ful in generating answers to questions, CrowS-Pairs and WinoGender to assess the stereotypes in LLMs, RealToxityPrompts to evaluate the extent to which LLMs generate toxic language, and HaluEval to test the ability of LLMs to recognize hallucination. As the test set of Real-Toxicity-Prompts is too large, we randomly sample 10000 examples from it for evaluation. We fol- low LLaMA to report the zero-shot performance, and compute the accuracy of identifying a claim as true for TruthfulQA, accuracy of recognizing biased sentences (high 67 TABLE 16: Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the Orange and Blue fonts denote the performance orders of the"
}